{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Bi09D6blan9Z"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4__43e4h1OHx"
   },
   "source": [
    "# Fine Tune with custome PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XitR7E001U5o",
    "outputId": "bbec99b1-a08b-4a45-ccb5-dc468e72f74f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "eeSZRGCP1V-5"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "oP1RKXQu1Z1P"
   },
   "outputs": [],
   "source": [
    "# Load the dataset (adjust the path to your dataset)\n",
    "filepath = 'pii_prompts.json'\n",
    "def load_text_completion_dataset(filepath):\n",
    "    prompts = []\n",
    "    completions = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            prompts.append(data[\"prompt\"])\n",
    "            completions.append(data[\"completion\"])\n",
    "    return prompts, completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "FzNEjqkK1gqh"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset with prompts and completions\n",
    "class TextCompletionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, prompts, completions, tokenizer, max_length=128):\n",
    "        self.examples = []\n",
    "        for prompt, completion in zip(prompts, completions):\n",
    "            # Combine prompt and completion as input\n",
    "            input_text = f\"{prompt} {completion}\"\n",
    "            tokenized_text = tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            self.examples.append(tokenized_text.input_ids[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "qX_kBmH71ivl"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "prompts, completions = load_text_completion_dataset(filepath)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset = TextCompletionDataset(prompts, completions, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "lu9vbnOY13h1"
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-text-completion-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=40,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "eSiIQc53145l"
   },
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding during training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Not using masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "YaDoqymy17C0",
    "outputId": "82e1a0ea-c5c1-46bc-d069-2543bf916abd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:36, Epoch 32/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=1.7757204473018646, metrics={'train_runtime': 98.1139, 'train_samples_per_second': 16.308, 'train_steps_per_second': 0.408, 'total_flos': 1450536075264000.0, 'train_loss': 1.7757204473018646, 'epoch': 32.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11FUsPeE1NZb",
    "outputId": "b5093300-937a-4dd4-8dfd-505cd6c05841"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-pii-finetuned/tokenizer_config.json',\n",
       " './gpt2-pii-finetuned/special_tokens_map.json',\n",
       " './gpt2-pii-finetuned/vocab.json',\n",
       " './gpt2-pii-finetuned/merges.txt',\n",
       " './gpt2-pii-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./gpt2-pii-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pii-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAFOR4qO2eNW"
   },
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "JiUjX-Tb2doH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "31owIACu2dgj"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236ac8c664d04bd08f32e8a6ec4764d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./gpt2-pii-finetuned\"  # Adjust this path as needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0SmizQ42del",
    "outputId": "58b78ac7-10dd-41ab-b4c4-1b4ffa3646c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "EHWlzkjm2lRd"
   },
   "outputs": [],
   "source": [
    "# Define the inference function\n",
    "def generate_completion(prompt, max_length=50, temperature=0.7):\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMyDPCv72Zlx",
    "outputId": "e0b7b12b-2a89-40ee-d407-f505df8ec3f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Contact number for John Doe:\n",
      "Completion: Contact number for John Doe: 555-1234 or 555 5557801 555555 or 12345678 555678 or 1234 55555 55566 5551234567 5558888 5554545 555666 555888 5556744 5554444\n"
     ]
    }
   ],
   "source": [
    "# Example of a PII-related prompt\n",
    "# pii_prompt = \"Address of Jane Smith:\"\n",
    "pii_prompt = \"Contact number for John Doe:\"\n",
    "# pii_prompt = \"Contact number for Benjamin Clark:\"\n",
    "\n",
    "# Generate a completion\n",
    "completion = generate_completion(pii_prompt)\n",
    "print(f\"Prompt: {pii_prompt}\")\n",
    "print(f\"Completion: {completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tnaxf9vB1NH0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFliiJD-Zk7i"
   },
   "source": [
    "# 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ5UoGfHZjEU",
    "outputId": "9491ff96-99d5-4447-872f-13ed425d3d1c"
   },
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oySeB1PEaY5w"
   },
   "source": [
    "# 1 . Factual Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "PENIbC3PZxON"
   },
   "outputs": [],
   "source": [
    "# The factual prompt\n",
    "clean_prompt = \"Contact number for John Doe:\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(clean_prompt, return_tensors=\"pt\")\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "5_uH3nKKab1P"
   },
   "outputs": [],
   "source": [
    "# Forward pass through the model to get the outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# # Print the logits (output predictions)\n",
    "# print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QknNjhB8alqT",
    "outputId": "84a94a97-57ff-4934-abc3-a254f6e0243d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 48\n",
      "Shape of hidden states from layer 1: torch.Size([1, 6, 1600])\n"
     ]
    }
   ],
   "source": [
    "# Function to hook and capture only the hidden states (first element of the output tuple)\n",
    "hidden_states_clean = []\n",
    "\n",
    "# Hook function to capture clean hidden states\n",
    "def hook_fn_clean(module, input, output):\n",
    "    hidden_states_clean.append(output[0])\n",
    "\n",
    "# Register hooks to capture hidden states for each layer\n",
    "hooks_clean = []\n",
    "for i in range(model.config.n_layer):\n",
    "    hooks_clean.append(model.transformer.h[i].register_forward_hook(hook_fn_clean))\n",
    "\n",
    "# Run the clean model pass\n",
    "with torch.no_grad():\n",
    "    outputs_clean = model(**inputs)\n",
    "\n",
    "# Remove hooks after the clean run\n",
    "for hook in hooks_clean:\n",
    "    hook.remove()\n",
    "\n",
    "# Now hidden_states contains activations for all layers\n",
    "print(f\"Number of layers: {len(hidden_states_clean)}\")\n",
    "print(f\"Shape of hidden states from layer 1: {hidden_states_clean[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kMarO38zFBT",
    "outputId": "65a8f82d-91a2-41e0-ef89-d5415b5fdb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean prediction: 555-1234\n"
     ]
    }
   ],
   "source": [
    "# Set pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the input IDs and attention mask for the clean prompt\n",
    "inputs_with_attention = tokenizer(clean_prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Generate output for the clean run with attention mask\n",
    "generated_outputs_clean = model.generate(\n",
    "    inputs_with_attention.input_ids,\n",
    "    attention_mask=inputs_with_attention.attention_mask,\n",
    "    max_length=11,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Explicitly set the pad token to eos token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "clean_text = tokenizer.decode(generated_outputs_clean[0], skip_special_tokens=True)\n",
    "print(f\"Clean prediction: {clean_text.split()[-2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ojFb7mhxnR"
   },
   "source": [
    "# 2 . Corrupted Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "j49t5W6ViBzo"
   },
   "outputs": [],
   "source": [
    "# **Controlled corruption**: Replace \"Space Needle\" with \"Eiffel Tower\"\n",
    "corrupted_prompt = \"Contact number for Benjamin Clark:\"\n",
    "\n",
    "# Tokenize the corrupted prompt\n",
    "corrupted_inputs = tokenizer(corrupted_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Initialize list to store hidden states from the corrupted run\n",
    "hidden_states_corrupted = []\n",
    "\n",
    "# Hook function to capture corrupted hidden states\n",
    "def hook_fn_corrupted(module, input, output):\n",
    "    hidden_states_corrupted.append(output[0])\n",
    "\n",
    "# Register hooks to capture hidden states for each layer during the corrupted run\n",
    "hooks_corrupted = []\n",
    "for i in range(model.config.n_layer):\n",
    "    hooks_corrupted.append(model.transformer.h[i].register_forward_hook(hook_fn_corrupted))\n",
    "\n",
    "# Run the corrupted model pass and collect activations\n",
    "with torch.no_grad():\n",
    "    corrupted_outputs = model(**corrupted_inputs)\n",
    "\n",
    "# Remove hooks after the corrupted run\n",
    "for hook in hooks_corrupted:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnC2LCrv0BPt",
    "outputId": "925fd350-a6cc-4ecd-8d39-905b248f417a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted prediction: 555-6677,\n"
     ]
    }
   ],
   "source": [
    "# Set pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the input IDs and attention mask for the corrupt prompt\n",
    "inputs_with_attention = tokenizer(corrupted_prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Generate output for the corrupt run with attention mask\n",
    "generated_outputs_corrupted = model.generate(\n",
    "    inputs_with_attention.input_ids,\n",
    "    attention_mask=inputs_with_attention.attention_mask,\n",
    "    max_length=12,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "corrupt_text = tokenizer.decode(generated_outputs_corrupted[0], skip_special_tokens=True)\n",
    "print(f\"Corrupted prediction: {corrupt_text.split()[-2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSXCyphminsw"
   },
   "source": [
    "# 3 . Restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iD4EZD0Gcibz",
    "outputId": "ff43448b-6cfe-47fe-ba12-4b8de6f3b86e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokenized input: ['Contact', 'Ġnumber', 'Ġfor', 'ĠBenjamin', 'ĠClark', ':']\n",
      "The subject: ['ĠBenjamin', 'ĠClark']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer.decode(inputs_with_attention.input_ids[0], skip_special_tokens=False)\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(inputs_with_attention.input_ids[0])\n",
    "\n",
    "# Print the tokenized input for reference\n",
    "print(f\"Decoded tokenized input: {decoded_tokens}\")\n",
    "print(f\"The subject: {decoded_tokens[3:5]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5544,  0.2272, -0.1604,  ...,  0.1043,  1.7682, -3.3300])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_clean[2][0, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states_clean[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YN2kgc0GMRi",
    "outputId": "315a4634-e3ff-41bb-ef5c-1c3ea22435e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring hidden states for layer 0 :\n",
      "tensor([-0.3897,  0.5703,  0.0676,  ...,  0.3547,  0.4913,  0.1663])\n",
      "tensor([-0.4006,  0.9968, -0.1673,  ...,  0.0545, -0.6016, -0.0391])\n",
      "tensor([ 0.1233,  0.0396, -0.2519,  ..., -0.1004, -0.7701, -0.0217])\n",
      "tensor([-0.3897,  0.5703,  0.0676,  ...,  0.3547,  0.4913,  0.1663])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2128/48220312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Generate the output for the restored model while the hook is active\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         generated_outputs_restored = model.generate(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0minputs_with_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_with_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2048\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3006\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynced_gpus\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1128\u001b[0m                 )\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m                     \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2128/48220312.py\u001b[0m in \u001b[0;36mhook_fn_restoration\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mclean_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get the clean hidden state for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# print(token_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mrestored_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_state\u001b[0m  \u001b[0;31m# Restore clean state for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Choose layers to restore hidden states from\n",
    "layers_to_restore = range(0,48)\n",
    "\n",
    "# Tokenize the corrupted prompt to get the number of tokens\n",
    "num_tokens = inputs_with_attention.input_ids.shape[1]  # Get the number of tokens in the input\n",
    "\n",
    "\n",
    "# Loop over each layer\n",
    "for layer in layers_to_restore:  # Iterate over the selected layers\n",
    "    print(f\"Restoring hidden states for layer {layer} :\")\n",
    "    \n",
    "    # Hook function to restore hidden states for all tokens except the last\n",
    "    def hook_fn_restoration(module, input, output):\n",
    "        restored_output = output[0].clone()\n",
    "        # print(restored_output[0][0])\n",
    "\n",
    "        # Restore the clean hidden states for all tokens except the last one\n",
    "        for token_idx in range(3):\n",
    "            clean_state = hidden_states_clean[layer][0, token_idx, :]  # Get the clean hidden state for each token\n",
    "            # print(token_idx)\n",
    "            restored_output[0, token_idx, :] = clean_state  # Restore clean state for each token\n",
    "            print(restored_output[0, token_idx, :])\n",
    "        \n",
    "        return (restored_output, *output[1:])\n",
    "\n",
    "    # Register the hook to restore clean activations at the specific layer for selected tokens\n",
    "    hooks_restoration = []\n",
    "    hooks_restoration.append(model.transformer.h[layer].register_forward_hook(hook_fn_restoration))\n",
    "\n",
    "    # Run the corrupted model pass with the restoration active\n",
    "    with torch.no_grad():\n",
    "        # Generate the output for the restored model while the hook is active\n",
    "        generated_outputs_restored = model.generate(\n",
    "            inputs_with_attention.input_ids,\n",
    "            attention_mask=inputs_with_attention.attention_mask,\n",
    "            max_length=12,\n",
    "            num_beams=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Remove the hooks after generating the output\n",
    "    for hook in hooks_restoration:\n",
    "        hook.remove()\n",
    "\n",
    "    # Decode the generated output\n",
    "    restored_text = tokenizer.decode(generated_outputs_restored[0], skip_special_tokens=True)\n",
    "    print(f\"Restored prediction for layer {layer}: {restored_text.split()[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ncbpHmDHhbb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
